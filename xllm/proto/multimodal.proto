syntax = "proto3";

option go_package = "jd.com/jd-infer/xllm;xllm";
package xllm.proto;

import "common.proto";
import "google/protobuf/struct.proto";
import "tensor.proto";
import "embedding_data.proto";

message ImageURL {
  string url = 1;
}

message VideoURL {
  string url = 1;
}

message AudioURL {
  string url = 1; 
}

message MMInputData {
  string type = 1;
  optional string text = 2;
  optional ImageURL image_url = 3;
  optional VideoURL video_url = 4;
  optional AudioURL audio_url = 5;
  optional Embedding image_embedding = 6;
  optional Embedding video_embedding = 7;
  optional Embedding audio_embedding = 8;
}

message MMChatMessage {
  // the role of the messages author. One of "system", "user", "assistant".
  optional string role = 1;

  // the content of the message. null for assistant messages with function calls.
  repeated MMInputData content = 2;

  repeated ToolCall tool_calls = 3;
  optional string tool_call_id = 4;
}

// Next Id: 27
message MMChatRequest {

  // ID of the model to use. You can use the ListModels endpoint to list available models.
  string model = 1;

  // A list of messages comprising the conversation so far.
  repeated MMChatMessage messages = 2;

  // the number of sequence to generate server-side and returns the "best". default = None
  // Results can't be streamed.
  // when used with n, best_of controls the number of candidate completions and n specifies
  // how many to return. best_of must be greater than or equal to n.
  optional uint32 best_of = 3;

  // TODO: Controls how the model responses to function calls. default = "none"
  // "none" - the model will ignore function calls.
  // "auto" - the model can pick between an end-user or calling a function.
  // string function_call = 4;

  // temperature of the sampling, between [0, 2]. default = 0.0
  // higher value will make the ouput more random, while a lower value will make it more deterministic.
  // it is recommended altering this or top_p but not both.
  optional float temperature = 5;

  // top_p sampling cutoff, between [0, 1.0]. default = 1.0
  optional float top_p = 6;

  // number of chat completion choices to generate for each input message. default = 1
  optional uint32 n = 7;

  // whether to stream partial completions back as they are generated. default = false
  optional bool stream = 8; 

  // up to 4 sequences where the API will stop generating further tokens.
  repeated string stop = 9;

  // the maximum number of tokens to generate in the chat completion. default = 16
  optional uint32 max_tokens = 10;

  // values between [-2.0, 2.0]. default = 0.0
  // Positive values penalize new tokens based on their existing in the text so far, increasing 
  // the model's likelihood to talk about new topics.
  optional float presence_penalty = 11;

  // values between [0.0, 2.0]. default = 0.0
  // Positive values penalize new tokens based on their existing frequency in the text so far,
  // decreasing the model's likelihood to repeat the same line verbatim.
  optional float frequency_penalty = 12;

  // TODO: logit_bias
  // modify the likelihood of specified tokens appearing in the completion.
  // map<int64, float> logit_bias = 13;

  // A unique identifier representing your end-user, which can help system to monitor and detect abuse.
  string user = 14;

  // the list of token ids where the API will stop generating further tokens.
  repeated int32 stop_token_ids = 16;

  // repetition penalty to penalize new tokens based on their occurence in the
  // text. values > 1.0 encourage the model to use new tokens, while values
  // < 1.0 encourage the model to repeat tokens. default = 1.0
  optional float repetition_penalty = 17;

  // top_k sampling cutoff, default = -1 (no cutoff)
  optional int64 top_k = 18;

  // whether to skip special tokens in the output. default = true
  optional bool skip_special_tokens = 19;

  // whether to ignore the end of sequence token. default = false.
  optional bool ignore_eos = 20;

  // whether to include the log probabilities of output tokens in the response. default = false
  optional bool logprobs = 21;

  // the number of log probabilities to include in the response, between [0, 20]. default = 0
  optional int32 top_logprobs = 22;

  // options for streaming response. Only set this when you set stream: true
  optional StreamOptions stream_options = 23;

  optional string request_id = 24;

  optional string service_request_id = 25;


  Routing routing = 26;

  repeated Tool tools = 27;

  optional string tool_choice = 28;

  optional bool offline = 29;

  optional int32 slo_ms = 30;

  // request priority. default = DEFAULT
  optional Priority priority = 31;

  optional int32 beam_width = 32;

  optional bool add_special_tokens = 33;

  optional google.protobuf.Struct chat_template_kwargs = 34;
}
